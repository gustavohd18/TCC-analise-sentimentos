{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Jupyter Notebook da Integradora 4\n",
    "## Aluno: Thiago Medina\n",
    "## Orientadora: Isabel H. Manssour"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abaixo, os imports necessário para a realização de todo o notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "#1\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install unidecode\n",
    "!{sys.executable} -m pip install tweepy\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install sklearn\n",
    "!{sys.executable} -m pip install openpyxl\n",
    "!{sys.executable} -m pip install xlrd\n",
    "!{sys.executable} -m pip install openpyxl\n",
    "!{sys.executable} -m pip install ssl\n",
    "\n",
    "import ssl\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in /Users/gustavoduarte/Library/Python/3.8/lib/python/site-packages (from nltk) (2021.7.6)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /Users/gustavoduarte/Library/Python/3.8/lib/python/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (4.61.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: unidecode in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tweepy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.9.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tweepy) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tweepy) (1.16.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.1.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.16 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.0.7)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from openpyxl) (1.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: xlrd in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.0.7)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from openpyxl) (1.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting ssl\n",
      "  Using cached ssl-1.16.tar.gz (33 kB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/wx/c047tw591yz4cytxbj_26lv40000gp/T/pip-install-v24m9vtx/ssl/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/wx/c047tw591yz4cytxbj_26lv40000gp/T/pip-install-v24m9vtx/ssl/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /private/var/folders/wx/c047tw591yz4cytxbj_26lv40000gp/T/pip-pip-egg-info-6_wgx1v5\n",
      "         cwd: /private/var/folders/wx/c047tw591yz4cytxbj_26lv40000gp/T/pip-install-v24m9vtx/ssl/\n",
      "    Complete output (6 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/private/var/folders/wx/c047tw591yz4cytxbj_26lv40000gp/T/pip-install-v24m9vtx/ssl/setup.py\", line 33\n",
      "        print 'looking for', f\n",
      "              ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print('looking for', f)?\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## O trabalho todo está separado em 3 partes:\n",
    "### 1- Coleta dos tweets\n",
    "### 2- Pré-processamento dos tweets.\n",
    "### 3- Treinamento e teste por parte do algoritmo\n",
    "\n",
    "Iniciamos com a coleta dos tweets:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Necessário possuir suas chaves próprias para a coleta de tweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "source": [
    "#2\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "#3\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tw.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "\n",
    "public_tweets = api.home_timeline()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query para a coleta de tweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Altere a query de pesquisa (pode-se utilizar operadores lógicos). Altere também a quantidade de tweets que deseja coletar."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#4\n",
    "query = '(#netflix) -filter:retweets'\n",
    "#query = '(#covid19) OR (#covid) OR (#coronavirus)  -filter:retweets'\n",
    "#query = '(#educacao AND (-@mpsnet AND -@mpsnet2)) -filter:retweets'\n",
    "\n",
    "cursor_tweets = tw.Cursor(api.search,\n",
    "            q=query,lang = 'pt',tweet_mode='extended').items(300)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizamos apenas a coluna com o texto dos tweets e a data de criação dos mesmos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#5\n",
    "tweets_dict = {}\n",
    "tweets_dict = tweets_dict.fromkeys(['created_at', 'full_text'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#6\n",
    "for tweet in cursor_tweets:\n",
    "    for key in tweets_dict.keys():\n",
    "        try:\n",
    "            twvalue = tweet._json[key]\n",
    "            tweets_dict[key].append(twvalue)\n",
    "        except KeyError:\n",
    "            twvalue = \"\"\n",
    "            if(tweets_dict[key] is None):\n",
    "                tweets_dict[key] = [twvalue]\n",
    "            else:\n",
    "                tweets_dict[key].append(twvalue)\n",
    "        except:\n",
    "            tweets_dict[key] = [twvalue]\n",
    "        #print(\"tweets_dict[key]: {} - tweet[key]: {}\".format(tweets_dict[key],  twvalue))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abaixo, listamos os tweets coletados."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#7\n",
    "tweets_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adicionamos esses tweets a um DataFrame, para poder manipular os dados."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#8\n",
    "dfTweets = pd.DataFrame.from_dict(tweets_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mostramos os primeiros 5 dados do nosso DataFrame:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#9\n",
    "dfTweets.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "E aqui, alteramos os nomes das colunas para português."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#10\n",
    "dfTweets.rename(columns={\"full_text\": \"Texto\", \"created_at\": \"Data de criação\"})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por fim, adicionamos uma coluna chamada \"Sentimento\", aonde será colocada a anotação manual do sentimento de cada um desses tweets, e salvamos o DataFrame em um arquivo Excel, aonde é mais simples a manipulação."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#11\n",
    "dfTweets.insert(2, \"Sentimento1\", \" \")\n",
    "dfTweets.insert(3, \"Sentimento2\", \" \")\n",
    "dfTweets.insert(4, \"SentimentoFinal\", \" \")\n",
    "dfTweets.to_excel(\"/Users/gustavoduarte/Desktop/TCC-analise-sentimentos-main/TweetsColetadosNetflix.xlsx\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A partir daqui, já deve ter sido realizada a anotação manual dos tweets!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tendo feita a anotação manual dos tweets, que no caso desse trabalho foi feito por 3 pessoas, adicionamos a tabela excel em uma variavel chamada tweets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "#12\n",
    "tweets= pd.read_excel('/Users/gustavoduarte/Desktop/TCC-analise-sentimentos-main/TabelaNetflix.xlsx', index_col=0,  engine='openpyxl') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abaixo, vemos como ficou nossa tabela.\n",
    "Lembrando que tweets classificados como 0 são negativos, como 1 são positivos e como 2 são neutros."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "#13\n",
    "tweets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             full_text  Sentimento1  \\\n",
       "0    #Grupos de #WhatsApp #Ecuador #Amistad #Ligar ...            2   \n",
       "1    Comecei a assistir #Demolidor na #Netflix e tô...            1   \n",
       "2    Pôster da 2º temporada de Love, Death &amp; Ro...            2   \n",
       "3    Alguem assistindo @quemmatousarabr no @Netflix...            2   \n",
       "4    Ansiedade ativa, laça logo a quinta temporada ...            2   \n",
       "..                                                 ...          ...   \n",
       "295  Las pelis y series que abandonan #Netflix hoy:...            2   \n",
       "296  ELA VOLTOU! Netflix faz boa ação e contrata Li...            2   \n",
       "297  Como eu sentia falta de uma série assim, vicia...            1   \n",
       "298  DATAS PARA A ÚLTIMA TEMPORADA DE \"LA CASA DE P...            2   \n",
       "299  Dudinha fala sobre a continuação de Anne de Gr...            2   \n",
       "\n",
       "     Sentimento1.1  SentimentoFinal  \n",
       "0                2                2  \n",
       "1                1                1  \n",
       "2                2                2  \n",
       "3                0                2  \n",
       "4                2                2  \n",
       "..             ...              ...  \n",
       "295              2                2  \n",
       "296              2                2  \n",
       "297              1                1  \n",
       "298              2                2  \n",
       "299              2                2  \n",
       "\n",
       "[300 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Sentimento1</th>\n",
       "      <th>Sentimento1.1</th>\n",
       "      <th>SentimentoFinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Grupos de #WhatsApp #Ecuador #Amistad #Ligar ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comecei a assistir #Demolidor na #Netflix e tô...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pôster da 2º temporada de Love, Death &amp;amp; Ro...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alguem assistindo @quemmatousarabr no @Netflix...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ansiedade ativa, laça logo a quinta temporada ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Las pelis y series que abandonan #Netflix hoy:...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>ELA VOLTOU! Netflix faz boa ação e contrata Li...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Como eu sentia falta de uma série assim, vicia...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>DATAS PARA A ÚLTIMA TEMPORADA DE \"LA CASA DE P...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Dudinha fala sobre a continuação de Anne de Gr...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 184
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A seguir, iniciaremos o trabalho de pré-processamento dos tweets dos nossos datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### O pré-processamento tem como objetivo deixar o texto presente nos tweets mais \"limpo\". Está sendo feito com apenas 1 dataset, apenas como exemplo, mas a técnica foi aplicada nos 3 datasets. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos utilizar um tweet como exemplo para que possa ser verificada o resultado de cada uma das tarefas de pré-processamento."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "#14\n",
    "exemploPreProcess = (tweets.at[57,'full_text'])\n",
    "print (exemploPreProcess)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apesar da história ter muitas reviravoltas e ser totalmente imprevisível, a 2ª temporada de #QuienMatoASara apresenta várias incongruências sem pés nem cabeça...\n",
      "\n",
      "Espreita a #review: https://t.co/MYGUazSwpb\n",
      "#seriesdatv #Netflix https://t.co/lR2ltdCTiP\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para a tokenização dos tweets (separar a frase em pequenos termos que façam sentido), utilizamos uma biblioteca específica para tweets. Ela auxilia na separação de emojis, hashtags, menções e outras questões próprias do Twitter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "#15\n",
    "tweet_tokenize = TweetTokenizer()\n",
    "tweet_tokenize.tokenize(exemploPreProcess)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Apesar',\n",
       " 'da',\n",
       " 'história',\n",
       " 'ter',\n",
       " 'muitas',\n",
       " 'reviravoltas',\n",
       " 'e',\n",
       " 'ser',\n",
       " 'totalmente',\n",
       " 'imprevisível',\n",
       " ',',\n",
       " 'a',\n",
       " '2ª',\n",
       " 'temporada',\n",
       " 'de',\n",
       " '#QuienMatoASara',\n",
       " 'apresenta',\n",
       " 'várias',\n",
       " 'incongruências',\n",
       " 'sem',\n",
       " 'pés',\n",
       " 'nem',\n",
       " 'cabeça',\n",
       " '...',\n",
       " 'Espreita',\n",
       " 'a',\n",
       " '#review',\n",
       " ':',\n",
       " 'https://t.co/MYGUazSwpb',\n",
       " '#seriesdatv',\n",
       " '#Netflix',\n",
       " 'https://t.co/lR2ltdCTiP']"
      ]
     },
     "metadata": {},
     "execution_count": 186
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A seguir, baixamos uma base de \"stopwords\" em português. Stopwords são palavras que não agregam sentido ao texto. Artigos e preposições são exemplos de stopwords."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "#16\n",
    "try:\n",
    "     _create_unverified_https_context =     ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "     pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "nltk.download('stopwords')\n",
    "def RemoveStopWords(tweet):\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    textInWords = []\n",
    "    for word in tweet.lower().split():\n",
    "        if (word not in stopwords) and (word.isdigit() == False): \n",
    "            textInWords.append(word)\n",
    "\n",
    "    res = ' '.join(map(str, textInWords))\n",
    "    return res"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gustavoduarte/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos ver abaixo como fica o tweet após a remoção de suas stopwords."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "#17\n",
    "RemoveStopWords(exemploPreProcess)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'apesar história ter muitas reviravoltas ser totalmente imprevisível, 2ª temporada #quienmatoasara apresenta várias incongruências pés cabeça... espreita #review: https://t.co/myguazswpb #seriesdatv #netflix https://t.co/lr2ltdctip'"
      ]
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para fazer a stemização de cada tweet (deixar apenas a raiz(radical) de cada termo), utilizamos uma biblioteca própria que nos ajuda a fazer isso. Essa bibloteca é específica para português. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "#18\n",
    "nltk.download('rslp')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     /Users/gustavoduarte/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 189
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "#19\n",
    "#stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tweet):\n",
    "    textInWords = []\n",
    "    for word in tweet.lower().split():\n",
    "        textInWords.append(stemmer.stem(word))\n",
    "    res = ' '.join(map(str, textInWords))\n",
    "    return res\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos ver abaixo como fica o tweet após a sua stemização."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "#20\n",
    "stem_tokens(exemploPreProcess)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'apes da histór ter muit reviravolt e ser total imprevisível, a 2ª tempor de #quienmatoas apresent vár incongru sem pé nem cabeça... espreit a #review: https://t.co/myguazswpb #seriesdatv #netflix https://t.co/lr2ltdctip'"
      ]
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abaixo, temos uma função de \"limpeza\". Nela, são excluídas quaisquer caracteres que não são letras. Também são excluídos links, hashtags e emojis. Todas as letras são passadas para minúsculo. Além disso, são excluídos dígitos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "#21\n",
    "def Limpa_dados(tweet): ##M MUDAR NOME DESSA FUNCAO\n",
    "    # remove links, pontos, virgulas,ponto e virgulas dos tweets E ACENTOS\n",
    "    tweet = unidecode(re.sub(r\"http\\S+\", \"\", tweet).lower().replace('.','')\n",
    "                      .replace(';','').replace('+','').replace(']','').replace('[','').replace('\\'','').replace(';','').replace('%','').replace('-',' ').replace(',',' ')\n",
    "                      .replace('\\n','').replace('\\\"','').replace(')','').replace('(','').replace('$','').replace('?','')\n",
    "                      .replace('_','').replace(':','').replace('!','').replace('|','').replace('/','').replace(')',''))\n",
    "    tweet = unidecode(re.sub(r\"@[A-Za-z0-9]+\", \"\", tweet))\n",
    "    tweet = unidecode(re.sub(r\"\\#[A-Za-z0-9]+\", \"\", tweet))\n",
    "    \n",
    "    tweet = unidecode(re.sub(r\"[0-9]+[A-Za-z0-9]+\", \"\", tweet)) #Remove digitos\n",
    "    tweet = tweet.replace('  ', ' ') #Corrige espaços feitos a mais.\n",
    "    tweet = tweet.replace('[?]', ' ') #Corrige outros errinhos.\n",
    "    return (tweet)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abaixo, vemos o tweets após a sua \"limpeza\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "source": [
    "#22\n",
    "Limpa_dados(exemploPreProcess)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'apesar da historia ter muitas reviravoltas e ser totalmente imprevisivel a temporada de apresenta varias incongruencias sem pes nem cabecaespreita a  '"
      ]
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por fim, executamos as 3 funções de pré-processamento e adicionamos a uma nova tabela Excel."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "#23\n",
    "#PreProcessa todos os tweets:\n",
    "\n",
    "qtdTweets = len(tweets)\n",
    "for tweet in range (qtdTweets):\n",
    "    tweets.at[tweet, 'full_text'] =  Limpa_dados(str(tweets.at[tweet, 'full_text']))\n",
    "    tweets.at[tweet, 'full_text'] =  RemoveStopWords(str(tweets.at[tweet, 'full_text']))\n",
    "    #tweets.at[tweet, 'full_text'] =  stem_tokens(str(tweets.at[tweet, 'full_text']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "#24\n",
    "tweets.to_excel(\"TabelaNetflixPreProcessados.xlsx\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Começaremos então o processo de treinamento por parte dos algoritmos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Primeiramente, pegamos os dados de treinamento e teste. Aqui estaremos treinando o algoritmo com os dados de covid (700 tweets) e testando com os dados de covid (300 tweets). Os dados vem de tabelas diferentes e são diferentes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "source": [
    "dadosTreino = pd.read_excel('TabelaCovidTreinoPreProcessados.xlsx').fillna(' ')\n",
    "dadosTeste = pd.read_excel('TabelaCovidTestePreProcessados.xlsx').fillna(' ')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'TabelaCovidTreinoPreProcessados.xlsx'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-c580882dd1f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdadosTreino\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TabelaCovidTreinoPreProcessados.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdadosTeste\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TabelaCovidTestePreProcessados.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows, ignore_workbook_corruption)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \"\"\"\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mfile_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;31m# We have to let unknown file formats pass through here, as some ancient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;31m# files that xlrd can parse don't start with the expected signature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36minspect_format\u001b[0;34m(path, content)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPEEK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'TabelaCovidTreinoPreProcessados.xlsx'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "dadosTreino.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           full_text  Sentimento1  \\\n",
       "0  noticia hora vai vacinar neste feriado entao f...            2   \n",
       "1  importante pais expliquem importancia medidas ...            2   \n",
       "2  veiculos contam sistema mensagem som informaco...            2   \n",
       "3  beijo tempos crise sara arapiun ednei arapiun ...            2   \n",
       "4  despacho intimados presidente jair bolsonaro m...            2   \n",
       "\n",
       "   Sentimento2  SentimentoFinal  \n",
       "0            2                2  \n",
       "1            2                2  \n",
       "2            2                2  \n",
       "3            2                2  \n",
       "4            2                2  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Sentimento1</th>\n",
       "      <th>Sentimento2</th>\n",
       "      <th>SentimentoFinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>noticia hora vai vacinar neste feriado entao f...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>importante pais expliquem importancia medidas ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>veiculos contam sistema mensagem som informaco...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beijo tempos crise sara arapiun ednei arapiun ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>despacho intimados presidente jair bolsonaro m...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "dadosTeste.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           full_text  Sentimento1  \\\n",
       "0  assumiu risco tomar morrer pesquisas demonstra...            0   \n",
       "1  india imagens hospital gtb nova delhi varios p...            0   \n",
       "2  brasil numero suspeitas casos confirmados nume...            2   \n",
       "3  boletim check up saudeola veja acoes melhorar ...            2   \n",
       "4  catorze estados distrito federal mantem ocupac...            0   \n",
       "\n",
       "   Sentimento2  SentimentoFinal  \n",
       "0            0                0  \n",
       "1            2                0  \n",
       "2            2                2  \n",
       "3            2                2  \n",
       "4            0                0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Sentimento1</th>\n",
       "      <th>Sentimento2</th>\n",
       "      <th>SentimentoFinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assumiu risco tomar morrer pesquisas demonstra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>india imagens hospital gtb nova delhi varios p...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brasil numero suspeitas casos confirmados nume...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boletim check up saudeola veja acoes melhorar ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>catorze estados distrito federal mantem ocupac...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos pegar os textos dos tweets de treinamento e teste e seus respectivos sentimentos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "tweetsParaTreino = dadosTreino['full_text'].values\n",
    "classesParaTreino = dadosTreino['SentimentoFinal'].values\n",
    "tweetsParaTeste = dadosTeste['full_text'].values\n",
    "classesParaTeste = dadosTeste['SentimentoFinal'].values\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizamos a técnica Holdout, separando os dados em 70% para treinamento e 30% para teste."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "qtdTweetsTreino = len(dadosTreino)\n",
    "qtdTweetsTreino"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "qtdTweetsTeste = len(dadosTeste)\n",
    "qtdTweetsTeste"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aqui, instanciamos o vetorizador dos tweets. Ele é responsável pela transformação dos dados textuais em um formato que o algoritmo entenda. É utilizada a técnica Bag Of Words, aonde as palavras viram colunas em uma tabela e cada tweet terá uma representação de 0's e 1's. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# Instancia o objeto que faz a vetorização dos dados de texto:\n",
    "tweet_tokenizer = TweetTokenizer() \n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abaixo, vetorizamos os dados de treinamento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "vect_tweetsTreino = vectorizer.fit_transform(tweetsParaTreino) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, faremos o treinamento do algoritmo. Utilizamos o algoritmo Multinomial Naive Bayes, o SVM."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "classificador = MultinomialNB()\n",
    "classificador.fit(vect_tweetsTreino, classesParaTreino)  \n",
    "\n",
    "classificadorSVM = svm.SVC(kernel='linear')\n",
    "classificadorSVM.fit(vect_tweetsTreino, classesParaTreino)\n",
    "\n",
    "classificadorLR = LogisticRegression(random_state=0).fit(vect_tweetsTreino, classesParaTreino)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Já tendo o algoritmo sido treinado, faremos a vetorização dos dados de teste."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "vect_tweetsTeste = vectorizer.transform(tweetsParaTeste) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abaixo, fazemos a predição dos dados de teste por parte do algoritmo já treinado."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "result = classificador.predict(vect_tweetsTeste)\n",
    "resultSVM = classificadorSVM.predict(vect_tweetsTeste)\n",
    "resultLR = classificadorLR.predict(vect_tweetsTeste)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Agora já possuímos o algoritmo treinado e os dados para verificar o resultado nos testes. Vamos prosseguir vendo o resultado das métricas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos ver a acurácia primeiramente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "acc = accuracy_score(classesParaTeste, result) * 100\n",
    "print(\"{}{} \".format(acc.round(2), \"%\"))\n",
    "\n",
    "acc2 = accuracy_score(classesParaTeste, resultSVM) * 100\n",
    "print(\"{}{} \".format(acc2.round(2), \"%\"))\n",
    "\n",
    "acc3 = accuracy_score(classesParaTeste, resultLR) * 100\n",
    "print(\"{}{} \".format(acc3.round(2), \"%\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "62.67% \n",
      "64.0% \n",
      "66.67% \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos verificar a matriz de confusão agora. Ela tem o objetivo de ver em que classes o algoritmo conseguiu classificar melhor."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "print (pd.crosstab(classesParaTeste, result, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTeste, resultSVM, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTeste, resultLR, rownames=['Real'], colnames=['Predito'], margins=True))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predito    0   1    2  All\n",
      "Real                      \n",
      "0         80   6   14  100\n",
      "1         18  23   18   59\n",
      "2         43  13   85  141\n",
      "All      141  42  117  300\n",
      "Predito   0   1    2  All\n",
      "Real                     \n",
      "0        58   6   36  100\n",
      "1        13  27   19   59\n",
      "2        25   9  107  141\n",
      "All      96  42  162  300\n",
      "Predito    0   1    2  All\n",
      "Real                      \n",
      "0         68   4   28  100\n",
      "1         10  26   23   59\n",
      "2         27   8  106  141\n",
      "All      105  38  157  300\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos verificar, por fim, as outras métricas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "print(classification_report(classesParaTeste, result, labels = [0,1,2]))\n",
    "\n",
    "print(classification_report(classesParaTeste, resultSVM, labels = [0,1,2]))\n",
    "\n",
    "print(classification_report(classesParaTeste, resultLR, labels = [0,1,2]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.80      0.66       100\n",
      "          1       0.55      0.39      0.46        59\n",
      "          2       0.73      0.60      0.66       141\n",
      "\n",
      "avg / total       0.64      0.63      0.62       300\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.58      0.59       100\n",
      "          1       0.64      0.46      0.53        59\n",
      "          2       0.66      0.76      0.71       141\n",
      "\n",
      "avg / total       0.64      0.64      0.63       300\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.68      0.66       100\n",
      "          1       0.68      0.44      0.54        59\n",
      "          2       0.68      0.75      0.71       141\n",
      "\n",
      "avg / total       0.67      0.67      0.66       300\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agora, faremos o treinamento com os dados da tabela de assuntos diversos (700 tweets). A tabela de teste segue sendo a mesma."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esse processo é semelhante ao feito anteriormente, então não há necessidade de explicar cada etapa."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "dadosTreinoDiversos = pd.read_excel('TabelaDiversosPreProcessados.xlsx').fillna(' ')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "dadosTreinoDiversos.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           full_text  Sentimento1  \\\n",
       "0  ante malestar coa falta dialogo medidas xunta ...            0   \n",
       "1             pra gastar podemos gastar pouco acesse            2   \n",
       "2  oposicao reforma tributaria desconhecimento lobby            0   \n",
       "3           importancia poupanca composicao setorial            1   \n",
       "4                               \"pib governo\" errado            0   \n",
       "\n",
       "   Sentimento2  SentimentoFinal  \n",
       "0            0                0  \n",
       "1            2                2  \n",
       "2            2                2  \n",
       "3            1                1  \n",
       "4            2                0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>Sentimento1</th>\n",
       "      <th>Sentimento2</th>\n",
       "      <th>SentimentoFinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ante malestar coa falta dialogo medidas xunta ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pra gastar podemos gastar pouco acesse</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oposicao reforma tributaria desconhecimento lobby</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>importancia poupanca composicao setorial</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"pib governo\" errado</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "tweetsParaTreinoDiversos = dadosTreinoDiversos['full_text'].values\n",
    "classesParaTreinoDiversos = dadosTreinoDiversos['SentimentoFinal'].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "qtdTweetsTreino = len(dadosTreinoDiversos)\n",
    "qtdTweetsTreino"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "vect_tweetsTreinoDiversos = vectorizer.fit_transform(tweetsParaTreinoDiversos)  #treina com diversos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "classificadorDiversos = MultinomialNB()\n",
    "classificadorDiversos.fit(vect_tweetsTreinoDiversos, classesParaTreinoDiversos) \n",
    "\n",
    "classificadorSVMDiv = svm.SVC(kernel='linear')\n",
    "classificadorSVMDiv.fit(vect_tweetsTreinoDiversos, classesParaTreinoDiversos)\n",
    "\n",
    "classificadorLRDiv = LogisticRegression(random_state=0).fit(vect_tweetsTreinoDiversos, classesParaTreinoDiversos)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "vect_tweetsTeste = vectorizer.transform(tweetsParaTeste) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "resultDiversos = classificadorDiversos.predict(vect_tweetsTeste)\n",
    "\n",
    "resultDiversosSVM = classificadorSVMDiv.predict(vect_tweetsTeste)\n",
    "\n",
    "resultDiversosLR = classificadorLRDiv.predict(vect_tweetsTeste)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "acc = accuracy_score(classesParaTeste, resultDiversos) * 100\n",
    "print(\"{}{} \".format(acc.round(2), \"%\"))\n",
    "\n",
    "acc2 = accuracy_score(classesParaTeste, resultDiversosSVM) * 100\n",
    "print(\"{}{} \".format(acc2.round(2), \"%\"))\n",
    "\n",
    "acc3 = accuracy_score(classesParaTeste, resultDiversosLR) * 100\n",
    "print(\"{}{} \".format(acc3.round(2), \"%\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "56.0% \n",
      "54.0% \n",
      "56.0% \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "print (pd.crosstab(classesParaTeste, resultDiversos, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTeste, resultDiversosSVM, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTeste, resultDiversosLR, rownames=['Real'], colnames=['Predito'], margins=True))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predito   0   1    2  All\n",
      "Real                     \n",
      "0        40  19   41  100\n",
      "1         6  30   23   59\n",
      "2        25  18   98  141\n",
      "All      71  67  162  300\n",
      "Predito   0   1    2  All\n",
      "Real                     \n",
      "0        32  14   54  100\n",
      "1         8  28   23   59\n",
      "2        19  20  102  141\n",
      "All      59  62  179  300\n",
      "Predito   0   1    2  All\n",
      "Real                     \n",
      "0        28  14   58  100\n",
      "1         4  30   25   59\n",
      "2        15  16  110  141\n",
      "All      47  60  193  300\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "print(classification_report(classesParaTeste, resultDiversos, labels = [0,1,2]))\n",
    "\n",
    "print(classification_report(classesParaTeste, resultDiversosSVM, labels = [0,1,2]))\n",
    "\n",
    "print(classification_report(classesParaTeste, resultDiversosLR, labels = [0,1,2]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.40      0.47       100\n",
      "          1       0.45      0.51      0.48        59\n",
      "          2       0.60      0.70      0.65       141\n",
      "\n",
      "avg / total       0.56      0.56      0.55       300\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.32      0.40       100\n",
      "          1       0.45      0.47      0.46        59\n",
      "          2       0.57      0.72      0.64       141\n",
      "\n",
      "avg / total       0.54      0.54      0.52       300\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.28      0.38       100\n",
      "          1       0.50      0.51      0.50        59\n",
      "          2       0.57      0.78      0.66       141\n",
      "\n",
      "avg / total       0.56      0.56      0.54       300\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agora faremos a classificação em tabelas binárias, aonde há apenas as classes positiva e negativa (1 e 0)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para tal, foram feitas cópias dos 3 datasets e excluídos os tweets neutros nessas cópias. Assim, faremos o treinamento e teste novamente nestes datasets.\n",
    "Este processo já foi explicado anteriormente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "dadosTreinoBin = pd.read_excel('TabelaCovidTreinoPreProcessadosBin.xlsx').fillna(' ')\n",
    "dadosTesteBin = pd.read_excel('TabelaCovidTestePreProcessadosBin.xlsx').fillna(' ')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "len(dadosTreinoBin)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "len(dadosTesteBin)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "tweetsParaTreinoBin = dadosTreinoBin['full_text'].values\n",
    "classesParaTreinoBin = dadosTreinoBin['SentimentoFinal'].values\n",
    "tweetsParaTesteBin = dadosTesteBin['full_text'].values\n",
    "classesParaTesteBin = dadosTesteBin['SentimentoFinal'].values\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# Instancia o objeto que faz a vetorização dos dados de texto:\n",
    "tweet_tokenizer = TweetTokenizer() \n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "vect_tweetsTreinoBin = vectorizer.fit_transform(tweetsParaTreinoBin) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "classificadorBin = MultinomialNB()\n",
    "classificadorBin.fit(vect_tweetsTreinoBin, classesParaTreinoBin)\n",
    "\n",
    "classificadorSVMBin = svm.SVC(kernel='linear')\n",
    "classificadorSVMBin.fit(vect_tweetsTreinoBin, classesParaTreinoBin)\n",
    "\n",
    "classificadorLRBin = LogisticRegression(random_state=0).fit(vect_tweetsTreinoBin, classesParaTreinoBin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "vect_tweetsTesteBin = vectorizer.transform(tweetsParaTesteBin) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "resultBin = classificadorBin.predict(vect_tweetsTesteBin)\n",
    "\n",
    "resultBinSVM = classificadorSVMBin.predict(vect_tweetsTesteBin)\n",
    "\n",
    "resultBinLR = classificadorLRBin.predict(vect_tweetsTesteBin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "acc = accuracy_score(classesParaTesteBin, resultBin) * 100\n",
    "print(\"{}{} \".format(acc.round(2), \"%\"))\n",
    "\n",
    "acc2 = accuracy_score(classesParaTesteBin, resultBinSVM) * 100\n",
    "print(\"{}{} \".format(acc2.round(2), \"%\"))\n",
    "\n",
    "acc3 = accuracy_score(classesParaTesteBin, resultBinLR) * 100\n",
    "print(\"{}{} \".format(acc3.round(2), \"%\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "81.13% \n",
      "79.25% \n",
      "78.62% \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "print (pd.crosstab(classesParaTesteBin, resultBin, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTesteBin, resultBinSVM, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTesteBin, resultBinLR, rownames=['Real'], colnames=['Predito'], margins=True))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predito    0   1  All\n",
      "Real                 \n",
      "0         94   6  100\n",
      "1         24  35   59\n",
      "All      118  41  159\n",
      "Predito    0   1  All\n",
      "Real                 \n",
      "0         91   9  100\n",
      "1         24  35   59\n",
      "All      115  44  159\n",
      "Predito    0   1  All\n",
      "Real                 \n",
      "0         95   5  100\n",
      "1         29  30   59\n",
      "All      124  35  159\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "print(classification_report(classesParaTesteBin, resultBin, labels = [0,1]))\n",
    "\n",
    "print(classification_report(classesParaTesteBin, resultBinSVM, labels = [0,1]))\n",
    "\n",
    "print(classification_report(classesParaTesteBin, resultBinLR, labels = [0,1]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.94      0.86       100\n",
      "          1       0.85      0.59      0.70        59\n",
      "\n",
      "avg / total       0.82      0.81      0.80       159\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.91      0.85       100\n",
      "          1       0.80      0.59      0.68        59\n",
      "\n",
      "avg / total       0.79      0.79      0.78       159\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.95      0.85       100\n",
      "          1       0.86      0.51      0.64        59\n",
      "\n",
      "avg / total       0.80      0.79      0.77       159\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Agora faremos o treinamento com o dataset de assuntos diversos sem os tweets neutros."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "dadosTreinoDiversosBin = pd.read_excel('TabelaDiversosPreProcessadosBin.xlsx').fillna(' ')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "tweetsParaTreinoDiversosBin = dadosTreinoDiversosBin['full_text'].values\n",
    "classesParaTreinoDiversosBin = dadosTreinoDiversosBin['SentimentoFinal'].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "qtdTweetsTreinoBin = len(dadosTreinoDiversosBin)\n",
    "qtdTweetsTreinoBin"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "vect_tweetsTreinoDiversosBin = vectorizer.fit_transform(tweetsParaTreinoDiversosBin)  #treina com diversos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "classificadorDiversosBin = MultinomialNB()\n",
    "classificadorDiversosBin.fit(vect_tweetsTreinoDiversosBin, classesParaTreinoDiversosBin)\n",
    "\n",
    "classificadorDiversosSVMBin = svm.SVC(kernel='linear')\n",
    "classificadorDiversosSVMBin.fit(vect_tweetsTreinoDiversosBin, classesParaTreinoDiversosBin)\n",
    "\n",
    "classificadorDiversosLRBin = LogisticRegression(random_state=0).fit(vect_tweetsTreinoDiversosBin, classesParaTreinoDiversosBin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "vect_tweetsTesteBin = vectorizer.transform(tweetsParaTesteBin) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "resultDiversosBin = classificadorDiversosBin.predict(vect_tweetsTesteBin)\n",
    "\n",
    "resultDiversosBinSVM = classificadorDiversosSVMBin.predict(vect_tweetsTesteBin)\n",
    "\n",
    "resultDiversosBinLR = classificadorDiversosLRBin.predict(vect_tweetsTesteBin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "acc = accuracy_score(classesParaTesteBin, resultDiversosBin) * 100\n",
    "print(\"{}{} \".format(acc.round(2), \"%\"))\n",
    "\n",
    "acc2 = accuracy_score(classesParaTesteBin, resultDiversosBinSVM) * 100\n",
    "print(\"{}{} \".format(acc2.round(2), \"%\"))\n",
    "\n",
    "acc3 = accuracy_score(classesParaTesteBin, resultDiversosBinLR) * 100\n",
    "print(\"{}{} \".format(acc3.round(2), \"%\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "74.21% \n",
      "73.58% \n",
      "74.21% \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "print (pd.crosstab(classesParaTesteBin, resultDiversosBin, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTesteBin, resultDiversosBinSVM, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTesteBin, resultDiversosBinLR, rownames=['Real'], colnames=['Predito'], margins=True))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predito   0   1  All\n",
      "Real                \n",
      "0        69  31  100\n",
      "1        10  49   59\n",
      "All      79  80  159\n",
      "Predito   0   1  All\n",
      "Real                \n",
      "0        71  29  100\n",
      "1        13  46   59\n",
      "All      84  75  159\n",
      "Predito   0   1  All\n",
      "Real                \n",
      "0        68  32  100\n",
      "1         9  50   59\n",
      "All      77  82  159\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "print(classification_report(classesParaTesteBin, resultDiversosBin, labels = [0,1]))\n",
    "\n",
    "print(classification_report(classesParaTesteBin, resultDiversosBinSVM, labels = [0,1]))\n",
    "\n",
    "print(classification_report(classesParaTesteBin, resultDiversosBinLR, labels = [0,1]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.69      0.77       100\n",
      "          1       0.61      0.83      0.71        59\n",
      "\n",
      "avg / total       0.78      0.74      0.75       159\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.71      0.77       100\n",
      "          1       0.61      0.78      0.69        59\n",
      "\n",
      "avg / total       0.76      0.74      0.74       159\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.68      0.77       100\n",
      "          1       0.61      0.85      0.71        59\n",
      "\n",
      "avg / total       0.78      0.74      0.75       159\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Por fim, vamos validar os resultados realizando o treinamento com os tweets de assuntos diversos e o teste com os tweets #netflix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esta etapa de treinamento/teste é como as anteriores, logo, não há necessidade de explicar cada passo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "dadosTreinoDiversos = pd.read_excel('TabelaDiversosPreProcessados.xlsx').fillna(' ')\n",
    "dadosTesteNetflix = pd.read_excel('TabelaNetflixPreProcessados.xlsx').fillna(' ')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "tweetsParaTreinoDiv = dadosTreinoDiversos['full_text'].values\n",
    "classesParaTreinoDiv = dadosTreinoDiversos['SentimentoFinal'].values\n",
    "tweetsParaTesteNetflix = dadosTesteNetflix['full_text'].values\n",
    "classesParaTesteNetflix = dadosTesteNetflix['SentimentoFinal'].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "qtdTweetsTreino = len(dadosTreinoDiversos)\n",
    "qtdTweetsTreino"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "qtdTweetsTeste = len(dadosTesteNetflix)\n",
    "qtdTweetsTeste"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "# Instancia o objeto que faz a vetorização dos dados de texto:\n",
    "tweet_tokenizer = TweetTokenizer() \n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "vect_tweetsTreinoDiv = vectorizer.fit_transform(tweetsParaTreinoDiv)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 47)\t1\n",
      "  (0, 2567)\t1\n",
      "  (0, 2521)\t1\n",
      "  (0, 1576)\t2\n",
      "  (0, 2574)\t1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "classificadorValid = MultinomialNB()\n",
    "classificadorValid.fit(vect_tweetsTreinoDiv, classesParaTreinoDiv)\n",
    "\n",
    "classificadorValidSVM = svm.SVC(kernel='linear')\n",
    "classificadorValidSVM.fit(vect_tweetsTreinoDiv, classesParaTreinoDiv)\n",
    "\n",
    "classificadorValidLR = LogisticRegression(random_state=0).fit(vect_tweetsTreinoDiv, classesParaTreinoDiv)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "vect_tweetsTesteNetflix = vectorizer.transform(tweetsParaTesteNetflix) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "resultValid = classificadorValid.predict(vect_tweetsTesteNetflix)\n",
    "\n",
    "resultValidSVM = classificadorValidSVM.predict(vect_tweetsTesteNetflix)\n",
    "\n",
    "resultValidLR = classificadorValidLR.predict(vect_tweetsTesteNetflix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "acc = accuracy_score(classesParaTesteNetflix, resultValid) * 100\n",
    "print(\"{}{} \".format(acc.round(2), \"%\"))\n",
    "\n",
    "acc2 = accuracy_score(classesParaTesteNetflix, resultValidSVM) * 100\n",
    "print(\"{}{} \".format(acc2.round(2), \"%\"))\n",
    "\n",
    "acc3 = accuracy_score(classesParaTesteNetflix, resultValidLR) * 100\n",
    "print(\"{}{} \".format(acc3.round(2), \"%\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "55.67% \n",
      "56.67% \n",
      "58.67% \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "print (pd.crosstab(classesParaTesteNetflix, resultValid, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTesteNetflix, resultValidSVM, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTesteNetflix, resultValidLR, rownames=['Real'], colnames=['Predito'], margins=True))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predito   0   1    2  All\n",
      "Real                     \n",
      "0        15   3   44   62\n",
      "1         7   9   49   65\n",
      "2        13  17  143  173\n",
      "All      35  29  236  300\n",
      "Predito   0   1    2  All\n",
      "Real                     \n",
      "0        11   0   51   62\n",
      "1         2   8   55   65\n",
      "2         6  16  151  173\n",
      "All      19  24  257  300\n",
      "Predito   0   1    2  All\n",
      "Real                     \n",
      "0         8   0   54   62\n",
      "1         1   5   59   65\n",
      "2         3   7  163  173\n",
      "All      12  12  276  300\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "print(classification_report(classesParaTesteNetflix, resultValid, labels = [0,1,2]))\n",
    "\n",
    "print(classification_report(classesParaTesteNetflix, resultValidSVM, labels = [0,1,2]))\n",
    "\n",
    "print(classification_report(classesParaTesteNetflix, resultValidLR, labels = [0,1,2]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.24      0.31        62\n",
      "          1       0.31      0.14      0.19        65\n",
      "          2       0.61      0.83      0.70       173\n",
      "\n",
      "avg / total       0.51      0.56      0.51       300\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.18      0.27        62\n",
      "          1       0.33      0.12      0.18        65\n",
      "          2       0.59      0.87      0.70       173\n",
      "\n",
      "avg / total       0.53      0.57      0.50       300\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.13      0.22        62\n",
      "          1       0.42      0.08      0.13        65\n",
      "          2       0.59      0.94      0.73       173\n",
      "\n",
      "avg / total       0.57      0.59      0.49       300\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Abaixo, o treinamento/teste aonde sem tweets neutros."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "dadosTreinoDiversosBin = pd.read_excel('TabelaDiversosPreProcessadosBin.xlsx').fillna(' ')\n",
    "dadosTesteNetflixBin = pd.read_excel('TabelaNetflixPreProcessadosBin.xlsx').fillna(' ')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "tweetsParaTreinoDiversosBin = dadosTreinoDiversosBin['full_text'].values\n",
    "classesParaTreinoDiversosBin = dadosTreinoDiversosBin['SentimentoFinal'].values\n",
    "tweetsParaTesteNetflixBin = dadosTesteNetflixBin['full_text'].values\n",
    "classesParaTesteNetflixBin = dadosTesteNetflixBin['SentimentoFinal'].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "# Instancia o objeto que faz a vetorização dos dados de texto:\n",
    "tweet_tokenizer = TweetTokenizer() \n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "vect_tweetsTreinoBin = vectorizer.fit_transform(tweetsParaTreinoDiversosBin) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "classificadorBinValid = MultinomialNB()\n",
    "classificadorBinValid.fit(vect_tweetsTreinoBin, classesParaTreinoDiversosBin)\n",
    "\n",
    "classificadorBinValidSVM = svm.SVC(kernel='linear')\n",
    "classificadorBinValidSVM.fit(vect_tweetsTreinoBin, classesParaTreinoDiversosBin)\n",
    "\n",
    "classificadorBinValidLR = LogisticRegression(random_state=0).fit(vect_tweetsTreinoBin, classesParaTreinoDiversosBin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "vect_tweetsTesteNetflixBin = vectorizer.transform(tweetsParaTesteNetflixBin) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "resultValidBin = classificadorBinValid.predict(vect_tweetsTesteNetflixBin)\n",
    "\n",
    "resultValidBinSVM = classificadorBinValidSVM.predict(vect_tweetsTesteNetflixBin)\n",
    "\n",
    "resultValidBinLR = classificadorBinValidLR.predict(vect_tweetsTesteNetflixBin)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "acc = accuracy_score(classesParaTesteNetflixBin, resultValidBin) * 100\n",
    "print(\"{}{} \".format(acc.round(2), \"%\"))\n",
    "\n",
    "acc2 = accuracy_score(classesParaTesteNetflixBin, resultValidBinSVM) * 100\n",
    "print(\"{}{} \".format(acc2.round(2), \"%\"))\n",
    "\n",
    "acc3 = accuracy_score(classesParaTesteNetflixBin, resultValidBinLR) * 100\n",
    "print(\"{}{} \".format(acc3.round(2), \"%\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "68.5% \n",
      "65.35% \n",
      "66.14% \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "print (pd.crosstab(classesParaTesteNetflixBin, resultValidBin, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTesteNetflixBin, resultValidBinSVM, rownames=['Real'], colnames=['Predito'], margins=True))\n",
    "\n",
    "print (pd.crosstab(classesParaTesteNetflixBin, resultValidBinLR, rownames=['Real'], colnames=['Predito'], margins=True))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predito   0   1  All\n",
      "Real                \n",
      "0        44  18   62\n",
      "1        22  43   65\n",
      "All      66  61  127\n",
      "Predito   0   1  All\n",
      "Real                \n",
      "0        35  27   62\n",
      "1        17  48   65\n",
      "All      52  75  127\n",
      "Predito   0   1  All\n",
      "Real                \n",
      "0        33  29   62\n",
      "1        14  51   65\n",
      "All      47  80  127\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "print(classification_report(classesParaTesteNetflixBin, resultValidBin, labels = [0,1]))\n",
    "\n",
    "print(classification_report(classesParaTesteNetflixBin, resultValidBinSVM, labels = [0,1]))\n",
    "\n",
    "print(classification_report(classesParaTesteNetflixBin, resultValidBinLR, labels = [0,1]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.71      0.69        62\n",
      "          1       0.70      0.66      0.68        65\n",
      "\n",
      "avg / total       0.69      0.69      0.68       127\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.56      0.61        62\n",
      "          1       0.64      0.74      0.69        65\n",
      "\n",
      "avg / total       0.66      0.65      0.65       127\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.53      0.61        62\n",
      "          1       0.64      0.78      0.70        65\n",
      "\n",
      "avg / total       0.67      0.66      0.66       127\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "###### Obrigado por olhar o meu notebook! Fique livre para copiá-lo, altera-lo e utiliza-lo em seus estudos/projetos "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}